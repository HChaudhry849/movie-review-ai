#
# 1. Add a entrypoint script in the root folder, this must create a training done file 
# 2. Update the docker file so that it executes the entrypoint script
# 3. Test it out 
# 4. Finally add dashboard 
# 5. Use artifacts to persist data in github actions
# 6. Update README with instructions on how to run locally and in CI/CD
# 7. Test on github actions
# WARNING: Watchout for CURL commands in healthchecks, ensure curl is installed in the respective dockerfiles
#
services:

    ollama-server:
        build:
            context: ./ollama_runtime
            dockerfile: Dockerfile
        ports:
            - "11434:11434"
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
            interval: 30s
            timeout: 10s
            retries: 3
            start_period: 30s
    
    ollama-service:
        build:
            context: ./ollama
            dockerfile: Dockerfile
        volumes:
            - ./data:/app/data
        depends_on:
            ollama-server:
                condition: service_healthy
        restart: on-failure
        environment:
            - OLLAMA_HOST=http://ollama-server:11434
        healthcheck:
            test: ["CMD", "test", "-f", "/app/data/training.done"]
            interval: 5s
            timeout: 3s
            retries: 20
    
    training-service:
        build:
            context: .
            dockerfile: Dockerfile
        volumes:
            # -------------------------------
            # Persistent Data Volumes
            #
            # We mount local folders into the container to **persist dynamic data**:
            #
            # 1. ./data:/app/data
            #    - Stores training CSVs and other shared data generated by ollama-service.
            #    - Training-service reads this data to perform training.
            #
            # 2. ./evaluate:/app/evaluate
            #    - Contains evaluation scripts or results generated by training-service.
            #
            # 3. ./models:/app/models
            #    - Stores trained model files produced by training-service.
            #
            # Why volumes are important:
            # - Any dynamic data generated inside the container will be **lost** if not persisted.
            # - If we were to COPY ./data, ./evaluate, or ./models in the Dockerfile, 
            #   the container would overwrite the dynamic content **every time it builds**, 
            #   destroying newly generated files.
            # - Volumes ensure that data survives container restarts and rebuilds.
            #
            # How it works locally:
            # - Changes inside /app/data, /app/evaluate, or /app/models are reflected on your host machine.
            # - Changes on the host machine are reflected in the container immediately.
            # - volumes live outside the container lifecycle.
            #
            # How to adapt for CI/CD (GitHub Actions):
            # - GitHub runners are ephemeral and do not persist local folders.
            # - Use Docker named volumes or upload/download artifacts using `actions/upload-artifact`.
            # - This allows the dynamic data (CSV, models, evaluation results) to persist between workflow steps.
            # -------------------------------
            - ./data:/app/data
            - ./evaluate:/app/evaluate
            - ./models:/app/models
        depends_on:
            ollama-service:
                condition: service_healthy
        healthcheck:
            #Simple healthcheck similar to ollama-service to ensure training-service is finished.
            #We must create a file /app/data/training.complete when training is done. This is a flag to confirm the process is done.
            #Create this file as part of a entrypoint script in the root folder.
            test: ["CMD", "test", "-f", "/app/data/training.complete"]
            interval: 5s
            timeout: 3s
            retries: 20
        
    flask-service:
        build:
            context: ./app
            dockerfile: Dockerfile
        ports:
            - "5000:5000"
        volumes:
            - ./models:/app/models
        # Flask service depends on the training service to ensure models are available
        depends_on:
            training-service:
                condition: service_healthy
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
            interval: 5s
            timeout: 3s
            retries: 20
    
    test:
        build:
            context: ./test
            dockerfile: Dockerfile
        depends_on:
        flask-service:
            condition: service_healthy