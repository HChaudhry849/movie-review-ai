#⚡ Best-practice recommendations
#Keep seed-service idempotent, safe to run multiple times.
#Ensure all dependent services wait for the seed completion:
#Either via healthchecks (e.g., /app/data/training.done or a seed.complete file),
#Or by manually orchestrating (docker-compose run seed-service first).
#Avoid hard-coded inter-container assumptions; rely on volumes and readiness checks.
#Continue to use one service per responsibility — this is good.
#✅ Conclusion
#Mostly follows best practices, especially with volumes, separation, and healthchecks.
#The only minor gap is the seed-service dependency: you need a clear guarantee it runs before others start.
#Otherwise, your services are modular, isolated, and volume-backed — aligned with Docker best practices.

services:

    seed-service:
        build:
            context: .
            dockerfile: Dockerfile
        volumes:
            - shared-data:/app/data
            - shared-evaluate:/app/evaluate
            - shared-models:/app/models

    ollama-server:
        build:
            context: ./ollama_runtime
            dockerfile: Dockerfile
        ports:
            - "11434:11434"
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
            interval: 30s
            timeout: 10s
            retries: 3
            start_period: 30s
    
    ollama-service:
        build:
            context: ./ollama
            dockerfile: Dockerfile
        volumes:
            - shared-data:/app/data
        depends_on:
            ollama-server:
                condition: service_healthy
        restart: on-failure
        environment:
            - OLLAMA_HOST=http://ollama-server:11434
        healthcheck:
            test: ["CMD", "test", "-f", "/app/data/training.done"]
            interval: 5s
            timeout: 3s
            retries: 20
    
    training-service:
        build:
            context: ./app/train
            dockerfile: Dockerfile
        volumes:
            # -------------------------------
            # Persistent Data Volumes
            #
            # We mount local folders into the container to **persist dynamic data**:
            #
            # 1. ./data:/app/data
            #    - Stores training CSVs and other shared data generated by ollama-service.
            #    - Training-service reads this data to perform training.
            #
            # 2. ./evaluate:/app/evaluate
            #    - Contains evaluation scripts or results generated by training-service.
            #
            # 3. ./models:/app/models
            #    - Stores trained model files produced by training-service.
            #
            # Why volumes are important:
            # - Any dynamic data generated inside the container will be **lost** if not persisted.
            # - If we were to COPY ./data, ./evaluate, or ./models in the Dockerfile, 
            #   the container would overwrite the dynamic content **every time it builds**, 
            #   destroying newly generated files.
            # - Volumes ensure that data survives container restarts and rebuilds.
            #
            # How it works locally:
            # - Changes inside /app/data, /app/evaluate, or /app/models are reflected on your host machine.
            # - Changes on the host machine are reflected in the container immediately.
            # - volumes live outside the container lifecycle.
            #
            # How to adapt for CI/CD (GitHub Actions):
            # - GitHub runners are ephemeral and do not persist local folders.
            # - Use Docker named volumes or upload/download artifacts using `actions/upload-artifact`.
            # - This allows the dynamic data (CSV, models, evaluation results) to persist between workflow steps.
            # -------------------------------
            - shared-data:/app/data
            - shared-evaluate:/app/evaluate
            - shared-models:/app/models
        healthcheck:
            #Simple healthcheck similar to ollama-service to ensure training-service is finished.
            #We must create a file /app/data/training.complete when training is done. This is a flag to confirm the process is done.
            #Create this file as part of a entrypoint script in the root folder.
            test: ["CMD", "test", "-f", "/app/data/training.complete"]
            interval: 5s
            timeout: 3s
            retries: 20
        
    flask-service:
        build:
            context: ./app/flask_app
            dockerfile: Dockerfile
        ports:
            - "5000:5000"
        volumes:
            - shared-models:/app/models
        # Flask service depends on the training service to ensure models are available
        depends_on:
            training-service:
                condition: service_healthy
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
            interval: 5s
            timeout: 3s
            retries: 20
    
    dashboard-service:
        build:
            context: ./app/dashboard
            dockerfile: Dockerfile
        volumes:
            - shared-evaluate:/app/evaluate
        ports:
            - "8501:8501"
        depends_on:
            flask-service:
                condition: service_healthy
    
    test:
        build:
            context: ./test
            dockerfile: Dockerfile
        depends_on:
            flask-service:
                condition: service_healthy
    
volumes:
    shared-data:
    shared-evaluate:
    shared-models: